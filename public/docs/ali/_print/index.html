<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link rel="canonical" type="text/html" href="http://localhost:1313/docs/ali/">
<link rel="alternate" type="application/rss&#43;xml" href="http://localhost:1313/docs/ali/index.xml">
<meta name="robots" content="noindex, nofollow">


<link rel="shortcut icon" href="/favicons/favicon.ico" >
<link rel="apple-touch-icon" sizes="57x57" href="/favicons/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/favicons/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/favicons/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/favicons/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/favicons/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/favicons/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/favicons/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/favicons/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="/favicons/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicons/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
<link rel="manifest" href="/favicons/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/favicons/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">
<title>ali | Goldydocs</title>
<meta name="description" content="Already installed? Play us a song, Navidrome!">
<meta property="og:url" content="http://localhost:1313/docs/ali/">
  <meta property="og:site_name" content="Goldydocs">
  <meta property="og:title" content="ali">
  <meta property="og:description" content="Already installed? Play us a song, Navidrome!">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="ali">
  <meta itemprop="description" content="Already installed? Play us a song, Navidrome!">
  <meta itemprop="datePublished" content="2024-10-16T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-10-16T00:00:00+00:00">
  <meta itemprop="wordCount" content="1">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="ali">
  <meta name="twitter:description" content="Already installed? Play us a song, Navidrome!">
<link href="/scss/main.css" rel="stylesheet">
<script
  src="https://code.jquery.com/jquery-3.7.1.min.js"
  integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g=="
  crossorigin="anonymous"></script>
<script defer
  src="https://unpkg.com/lunr@2.3.9/lunr.min.js"
  integrity="sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli"
  crossorigin="anonymous"></script>

  </head>
  <body class="td-section">
    <header>
      <nav class="td-navbar js-navbar-scroll" data-bs-theme="dark">
<div class="container-fluid flex-column flex-md-row">
  <a class="navbar-brand" href="/"><span class="navbar-brand__logo navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="Layer_1" viewBox="0 0 512 512" style="enable-background:new 0 0 512 512"><path style="fill:#0084ff" d="M256 10.449C120.275 10.449 10.449 120.276 10.449 256c0 135.725 109.826 245.551 245.551 245.551S501.551 391.725 501.551 256C501.551 120.276 391.725 10.449 256 10.449z"/><path style="fill:#fff" d="M256 168.229c-48.515.0-87.771 39.257-87.771 87.771s39.257 87.771 87.771 87.771 87.771-39.257 87.771-87.771S304.515 168.229 256 168.229z"/><path d="M437.061 74.94C388.735 26.615 324.434.0 256 0S123.265 26.615 74.939 74.94.0 187.566.0 256s26.614 132.735 74.939 181.061S187.566 512 256 512s132.735-26.614 181.061-74.939S512 324.434 512 256 485.386 123.266 437.061 74.94zM256 491.102C126.365 491.102 20.898 385.635 20.898 256S126.365 20.898 256 20.898 491.102 126.365 491.102 256 385.635 491.102 256 491.102z"/><path d="M354.22 256c0-54.158-44.061-98.22-98.22-98.22s-98.22 44.062-98.22 98.22 44.062 98.22 98.22 98.22 98.22-44.061 98.22-98.22zm-175.542.0c0-42.636 34.686-77.322 77.322-77.322s77.322 34.686 77.322 77.322S298.636 333.322 256 333.322 178.678 298.636 178.678 256z"/><path d="M285.257 256c0-16.132-13.124-29.257-29.257-29.257S226.743 239.868 226.743 256s13.125 29.257 29.257 29.257S285.257 272.133 285.257 256zm-37.616.0c0-4.609 3.75-8.359 8.359-8.359 4.609.0 8.359 3.75 8.359 8.359.0 4.609-3.75 8.359-8.359 8.359C251.391 264.359 247.641 260.609 247.641 256z"/><path d="M256 391.837c-5.771.0-10.449 4.678-10.449 10.449s4.678 10.449 10.449 10.449c41.897.0 81.265-16.294 110.853-45.883 29.587-29.587 45.881-68.955 45.881-110.852.0-5.771-4.678-10.449-10.449-10.449s-10.449 4.678-10.449 10.449C391.837 330.9 330.9 391.837 256 391.837z"/><path d="M449.306 245.551c-5.771.0-10.449 4.678-10.449 10.449.0 1.481-.018 2.958-.052 4.43-.136 5.769 4.43 10.557 10.199 10.692.085.002.168.003.252.003 5.656.0 10.307-4.517 10.442-10.202.039-1.637.059-3.279.059-4.925C459.755 250.229 455.077 245.551 449.306 245.551z"/><path d="M447.55 284.239c-5.656-1.145-11.168 2.52-12.308 8.178-17.107 84.854-92.489 146.44-179.242 146.44-5.771.0-10.449 4.678-10.449 10.449s4.678 10.449 10.449 10.449c47.353.0 93.467-16.573 129.847-46.668 35.9-29.697 60.717-71.084 69.881-116.54C456.868 290.891 453.207 285.38 447.55 284.239z"/><path d="M256 120.163c5.771.0 10.449-4.678 10.449-10.449S261.771 99.265 256 99.265c-41.897.0-81.265 16.294-110.853 45.883C115.559 174.735 99.265 214.103 99.265 256c0 5.771 4.678 10.449 10.449 10.449s10.449-4.678 10.449-10.449c0-74.9 60.937-135.837 135.837-135.837z"/><path d="M62.996 240.876c-5.759-.14-10.557 4.43-10.692 10.199-.039 1.637-.059 3.279-.059 4.925.0 5.771 4.678 10.449 10.449 10.449S73.143 261.771 73.143 256c0-1.481.018-2.958.052-4.43C73.331 245.8 68.765 241.013 62.996 240.876z"/><path d="M256 73.143c5.771.0 10.449-4.678 10.449-10.449S261.771 52.245 256 52.245c-47.353.0-93.467 16.573-129.847 46.668-35.9 29.697-60.717 71.084-69.881 116.54-1.14 5.657 2.521 11.168 8.178 12.308.697.141 1.392.208 2.076.208 4.872.0 9.232-3.426 10.232-8.386C93.864 134.729 169.247 73.143 256 73.143z"/></svg></span><span class="navbar-brand__name">Goldydocs</span></a>
  <div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link" href="/about/"><span>About</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/demo/"><span>Demo</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link active" href="/docs/"><span>Documentation</span></a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="/community/"><span>Community</span></a>
      </li>
      </ul>
  </div>
  <div class="d-none d-lg-block">
    <div class="td-search td-search--offline">
  <div class="td-search__icon"></div>
  <input
    type="search"
    class="td-search__input form-control"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
    
    data-offline-search-index-json-src="/offline-search-index.json"
    data-offline-search-base-href="/"
    data-offline-search-max-results="10"
  >
</div>
  </div>
</div>
</nav>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 ps-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/docs/ali/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">ali</h1>
<div class="lead">Already installed? Play us a song, Navidrome!</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-cd840cb4741d60545040a54cf965e880">1_入手工具</a></li>


    
  
    
    
	
<li>2: <a href="#pg-25d8510c939ba9753510f6c5f6dc4a19">2_构建本地大模型</a></li>


    
  
    
    
	
<li>3: <a href="#pg-78c91ffee44e5bfcc3a0851bed233043">3_ollama大模型构建</a></li>


    
  
    
    
	
<li>4: <a href="#pg-53ae680ef008e23e6c9bfe86508e67b9">4_基于AnythingLLM构建RAG</a></li>


    
  
    
    
	
<li>5: <a href="#pg-9cb00e52e7f02f2393d4936ac7ce5540">5_基于Huggingdist和Neo4j构建RAG</a></li>


    
  
    
    
	
<li>6: <a href="#pg-224ecc6e4d20e59f8fc1ec4333097cd9">6_关于LangChain的调研</a></li>


    
  
    
    
	
<li>7: <a href="#pg-d3734564d8e86d400fa1658422ed585b">7_基于Langchain和ollama构建的本地RAG雏形</a></li>


    
  
    
    
	
<li>8: <a href="#pg-750249da55139a8c5fa2e11520ded12a">8_基于ollama对RAG的调用来构建neo4j的知识图谱</a></li>


    
  
    
    
	
<li>9: <a href="#pg-35862ca271f0c20c95171af619a3df1b">9_Huggingface transformers文本生成</a></li>


    
  
    
    
	
<li>10: <a href="#pg-a17fb2f37c71e00be1119a67b908c897">10_x</a></li>


    
  

    </ul>


<div class="content">
      <p>开始分享吧</p>

</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-cd840cb4741d60545040a54cf965e880">1 - 1_入手工具</h1>
    
	<h1 id="一工具下载地址">一、工具下载地址</h1>
<p><a href="https://huggingface.co/models">https://huggingface.co/models</a></p>
<p><img alt="alt text" src="/docs/ali/image-1.png">
Huggingface被称为大模型界的github, 在这里用户可以分享和下载预训练各种开源大模型。这让用户无需重新训练模型就能使用最先进的自然语言处理模型。</p>
<p><a href="https://lmstudio.ai/">https://lmstudio.ai/</a></p>
<p>LM Studio支持多种模型的部署和微调，支持各种API调用</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-25d8510c939ba9753510f6c5f6dc4a19">2 - 2_构建本地大模型</h1>
    
	<p>下载 LM Studio
<img alt="alt text" src="/docs/ali/image-1.png"></p>
<p>构建本地大模型</p>
<p>下载gguf模型</p>
<p>法一：在App直接下载模型（本质是在Huggingface官网搜索下载，所以需要外网VPN环境</p>
<p><img alt="alt text" src="/docs/ali/image.png"></p>
<p>法二：魔搭下载gguf模型（不需要vpn环境下载模型的方法）</p>
<p><a href="https://www.modelscope.cn/">https://www.modelscope.cn/</a></p>
<p><img alt="alt text" src="/docs/ali/image-4.png"></p>
<p><img alt="alt text" src="/docs/ali/image-7.png"></p>
<p>配置大模型</p>
<p>gguf文件的存储文件夹格式是models\Publisher\Repository，同时设置本地模型文件夹是models。</p>
<p><img alt="alt text" src="/docs/ali/image-8.png"></p>
<p>本地API服务
<img alt="alt text" src="/docs/ali/image-2.png"></p>
<p><img alt="alt text" src="/docs/ali/image-9.png"></p>
<p>postman测试（测试json存放在md文件夹下）</p>
<p><img alt="alt text" src="/docs/ali/image-5.png"></p>
<p><img alt="alt text" src="/docs/ali/image-10.png"></p>
<p><img alt="alt text" src="/docs/ali/image-11.png"></p>
<p>python调用</p>
<p><img alt="alt text" src="/docs/ali/image-3.png"></p>
<p>messages中的content为返回值，通过completion.choices[0].message.content输出得到大模型返回内容</p>
<p><img alt="alt text" src="/docs/ali/image-6.png"></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-78c91ffee44e5bfcc3a0851bed233043">3 - 3_ollama大模型构建</h1>
    
	<p>一、LLama3 大模型本地部署安装</p>
<p>首先去mata官网下载ollama客户端</p>
<p>Ollama</p>
<p><a href="https://ollama.com/download">https://ollama.com/download</a></p>
<p><img alt="alt text" src="/docs/ali/image-12.png"></p>
<p>下载完成后，双击文件进行安装。安装成功后不会有反应。
<img alt="alt text" src="/docs/ali/image-13.png"></p>
<p>打开cmd控制台，键入ollama，如果如下图显示命令提示就说明安装已经成功</p>
<p><img alt="alt text" src="/docs/ali/image-14.png"></p>
<p>进入官网找到想要下载的模型名称</p>
<p><a href="https://ollama.com/library/llama3/tags">https://ollama.com/library/llama3/tags</a>。</p>
<p>如果使用的是普通电脑，建议选择8b（80亿参数）</p>
<p><img alt="alt text" src="/docs/ali/image-18.png"></p>
<p>查看自己电脑GPU</p>
<p>Windows设置 ——&raquo;&raquo;&gt; 更新和安全 ——&raquo;&raquo;&gt;</p>
<p><img alt="alt text" src="/docs/ali/image-15.png"></p>
<p><img alt="alt text" src="/docs/ali/image-16.png"></p>
<p><img alt="alt text" src="/docs/ali/image-17.png"></p>
<p>控制台中键入ollama run llama3:8b，会自动进行8B模型下载，该模型有80亿参数，普通电脑可以执行。如果想要70B可以修改为入ollama run llama3:8b。按回车键就开始下载并自动安装</p>
<p><img alt="alt text" src="/docs/ali/image-19.png"></p>
<p>当提示success时，说明已经成功安装了大模型。</p>
<p>接下来是可视化界面的安装</p>
<p>首先确保系统中已经安装了nodejs和git，如果没有可以自己去官网下载安装
<a href="https://nodejs.org/en">https://nodejs.org/en</a></p>
<p>打开cmd控制台，键入node -v，如果显示vxx.xx.x.就说明安装成功了。</p>
<p>在cmd控制台中键入</p>
<p>git clone <a href="https://github.com/ollama-webui/ollama-webui-lite.git">https://github.com/ollama-webui/ollama-webui-lite.git</a></p>
<p>添加资源镜像
在cmd控制台键入mkdir llama在当前路径下创建一个叫llama的文件夹，然后键入cd llama进入该文件夹下
在cmd控制台键入</p>
<p>git clone <a href="https://github.com/ollama-webui/ollama-webui-lite.git">https://github.com/ollama-webui/ollama-webui-lite.git</a></p>
<p>将web ui界面下载回来
在cmd控制台键入</p>
<p>cd ollama-webui-lite</p>
<p>进入web ui项目文件夹
在cmd控制台键入</p>
<p>npm install</p>
<p>安装环境与依赖
在cmd控制台键入</p>
<p>npm run dev</p>
<p>启动web ui系统</p>
<p><img alt="alt text" src="/docs/ali/image-20.png"></p>
<p><img alt="alt text" src="/docs/ali/image-21.png"></p>
<p>然后点击Select a model 设置模型，我们选中已经下载好的8b模型</p>
<p><img alt="alt text" src="/docs/ali/image-22.png"></p>
<p>现在就可以在下方输入框进行输入对话了</p>
<p><img alt="alt text" src="/docs/ali/image-23.png"></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-53ae680ef008e23e6c9bfe86508e67b9">4 - 4_基于AnythingLLM构建RAG</h1>
    
	<p>下载地址
<a href="https://anythingllm.com/">https://anythingllm.com/</a></p>
<p><img alt="alt text" src="/docs/ali/image-36.png"></p>
<p><img alt="alt text" src="/docs/ali/image-24.png"></p>
<p>选择LLM模型（必须是本地有的模型）
<img alt="alt text" src="/docs/ali/image-25.png"></p>
<p>选择向量数据库（默认）
<img alt="alt text" src="/docs/ali/image-26.png"></p>
<p>选择嵌入引擎（默认）
<img alt="alt text" src="/docs/ali/image-27.png"></p>
<p>1.创建一个专用工作区</p>
<p>有了不同的工作区，提供不同的数据，可以分别去做不同的角色工作</p>
<p><img alt="alt text" src="/docs/ali/image-28.png"></p>
<p>2.上传数据
<img alt="alt text" src="/docs/ali/image-31.png"></p>
<p>包括pdf、word、excel等多种格式
也可以使用链接上传
<img alt="alt text" src="/docs/ali/image-32.png"></p>
<p>嵌入向量数据库
<img alt="alt text" src="/docs/ali/image-34.png"></p>
<p>3.查看效果
<img alt="alt text" src="/docs/ali/image-33.png"></p>
<p><img alt="alt text" src="/docs/ali/image-35.png"></p>
<p>支持权限管理
<img alt="alt text" src="/docs/ali/image-29.png"></p>
<p>支持API调用
<img alt="alt text" src="/docs/ali/image-30.png"></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9cb00e52e7f02f2393d4936ac7ce5540">5 - 5_基于Huggingdist和Neo4j构建RAG</h1>
    
	<p>Neo4j的关键概念和特点
•Neo4j是一个开源的NoSQL图形存储数据库，可为应用程序提供支持ACID的后端。Neo4j的开发始于2003年，自2007年转变为开源图形数据库模型。程序员使用的是路由器和关系的灵活网络结构，而不是静态表，但是可以享受企业级质量数据库的所有好处。与关系数据库索引，对于许多应用程序，Neo4j可以提供数量级的性能优势。</p>
<p>Neo4j的安装与配置
Neo4j是基于Java的图形数据库，运行Neo4j需要启动JVM进程，因此在安装Neo4j前必须安装JAVA SE的JDK。从Oracle官方网站下载Java SE JDK，
地址为：
<a href="https://www.oracle.com/cn/java/technologies/javase-downloads.html">https://www.oracle.com/cn/java/technologies/javase-downloads.html</a></p>
<p><img alt="alt text" src="/docs/ali/image-37.png"></p>
<p>Java JDK安装
1.双击.exe安装包（当前的最新版本）
<img alt="alt text" src="/docs/ali/image-38.png"></p>
<p>2.根据向导安装
<img alt="alt text" src="/docs/ali/image-39.png"></p>
<p><img alt="alt text" src="/docs/ali/image-40.png"></p>
<p><img alt="alt text" src="/docs/ali/image-41.png"></p>
<p>3.配置环境变量
<img alt="alt text" src="/docs/ali/image-42.png"></p>
<p>编辑系统变量区的Path，点击新建，然后输入 %JAVA_HOME%\bin
<img alt="alt text" src="/docs/ali/image-43.png"></p>
<p>4.测试
打开命令提示符CMD(WIN+R,输入cmd)，输入 java -version，若提示Java的版本信息，则证明环境变量配置成功</p>
<p><img alt="alt text" src="/docs/ali/image-44.png"></p>
<p>Neo4j的安装和配置</p>
<p>1.下载安装包</p>
<p>官网下载地址：https://neo4j.com/</p>
<p>2.解压</p>
<p><img alt="alt text" src="/docs/ali/image-45.png"></p>
<p>3.配置环境变量(和Java 环境变量设置步骤一致)
将下载的压缩文件解压到系统合适的位置后需要创建主目录环境变量NEO4J_HOME，变量值设置为主目录路径。图是主目录路径为C:\Users\ChangjianPan\Downloads\neo4j-community-5.22.0-windows\neo4j-community-5.22.0
的环境变量。</p>
<p><img alt="alt text" src="/docs/ali/image-46.png"></p>
<p>编辑系统变量区的Path，点击新建，然后输入 %NEO4J_HOME%\bin</p>
<p><img alt="alt text" src="/docs/ali/image-47.png"></p>
<p>Neo4j服务器具有一个集成的浏览器，在一个运行的服务器实例上访问
http://localhost:7474/</p>
<p>打开浏览器，显示启动页面。</p>
<p><img alt="alt text" src="/docs/ali/image-48.png"></p>
<p>在如图命令行依靠neo4j cli命令操作数据库
<img alt="alt text" src="/docs/ali/image-49.png"></p>
<p>启动neo4j数据库服务器</p>
<p>通过使用neo4j.bat文件启动服务器，该文件存放在主目录的bin文件夹中
在cmd中，输入neo4j.bat console并回车</p>
<p><img alt="alt text" src="/docs/ali/image-78.png"></p>
<p>出现下图结果，则表示成功启动
<img alt="alt text" src="/docs/ali/image-79.png"></p>
<p>123456Aa</p>
<p>忘记neo4j密码怎么办</p>
<p>找到neo4j.config文件
<img alt="alt text" src="/docs/ali/image-80.png"></p>
<p>将 dbms.security.auth_enabled=false的注释去除</p>
<p><img alt="alt text" src="/docs/ali/image-81.png"></p>
<p>这样再次启动neo4j数据库服务器，以无密码身份登录neo4j数据库</p>
<p>然后在命令行窗口运行命令</p>
<p>ALTER USER neo4j SET PASSWORD &lsquo;新密码&rsquo;;</p>
<p><img alt="alt text" src="/docs/ali/image-82.png"></p>
<p>HuggingFists是什么</p>
<p>伴随着LLM日新月异的发展，业界对于LLM的落地思考逐渐聚焦到两个方向上。一是RAG(Retrieval-Augmented Generation),检索增强生成；一是Agents, 智能体。我们这个系列的文章也将围绕这两个应用方向介绍如何使用HuggingFists进行落地实现。其社区版可通过以下链接获得(<a href="https://github.com/Datayoo/HuggingFists">https://github.com/Datayoo/HuggingFists</a>)。</p>
<p>什么是RAG
RAG，检索增强生成，即大模型LLM在回答问题或生成文本时，通过外挂其他数据源的方式来增强 LLM 的能力。使用外挂数据源检索出相关信息，然后基于这些检索出的信息进行回答或生成文本，从而提高回答的质量。外挂数据源可以是向量数据库、文档数据库、搜索引擎或应用系统等。RAG技术使得LLM在垂直领域应用时，不需要重新训练整个大模型，只需要外挂上相关知识库就可提供问答服务。从而节省了大模型的实施成本，同时提高了大模型在垂直领域的应用的时效性、准确性和安全性。</p>
<p>RAG工程面临的问题</p>
<p><img alt="alt text" src="/docs/ali/image-87.png"></p>
<p>基于向量数据库的RAG结构图</p>
<p>上图为基于向量数据库实现RAG的典型结构。除该结构外，业界还有基于ElasticSearch或二者共用的工程结构。本节将主要探讨所有工程结构都会面临的两个通用问题，一是RAG工程的技术选型问题；二是RAG工程的垂域数据接入问题。</p>
<p>RAG工程选定工程结构后，仍有大量的技术方案需要根据工程环境进行技术选型。如工程为云环境，则可以使用云端的向量数据库、LLM及Embedding技术；若工程要求本地化部署，且不具备外网服务访问条件，则需要选择可本地部署的向量库、LLM及Embedding等技术。如今，可供选择的技术方案有很多，工程师需要消耗不小的精力来试验并搭建出合适的选择。另外，如何优化检索也需要不断尝试。需要找出合适的方法来提高检索结果与问题的相关性，控制检索结果的大小以适配LLM的Context限制及token消耗。在找到合适的方案前，这些都将消耗不小的精力。
RAG工程待处理的垂域数据种类、格式繁多，有效提取这些数据并存入检索系统工作量巨大。这类工作其本质就是一个ETL过程。与传统的数据分析类项目相似，垂域数据预处理的工作量将占据整个项目的70%～80%。差别在于，传统数据分析项目面向的是结构化数据，而RAG项目面向的则更多是非结构化的数据，且情况更复杂。如相关数据是以Json或XML等格式描述的，且被存储在了如HBase、MongoDB等数据库中；文档结构比较复杂，无法简单通过文本抽取获得文档内容，而是需要文本抽取+OCR识别等多种技术相结合等。这些情况无疑都会加大RAG工程的落地成本。</p>
<p>解决上述问题，相信绝大多数LLM的关注者会首先想到LainChain。的确，LainChain无疑是目前实现LLM应用最炙手可热的框架。其采用Python语言作为开发语言，简单易用，拥有大量拥趸。自诞生以来，发展迅猛，且仍在不断完善中。</p>
<p>但有如我们在上述问题的描述中所说，RAG工程技术选型时有很多试验工作，垂域数据接入时有复杂场景需要适配。这些工作都存有更灵活、更便捷、更易用的潜在需求。特别是垂域数据接入，在过去几十年的传统数据接入工程中，存在大量低代码定义数据处理Pipeline的工具，如Kettle、StreamSets等。这些工具为复杂场景的垂域数据接入提供了很好的方案指导，可大大提高数据接入的效率，降低数据接入人员的编程能力要求。</p>
<p>基于此，HuggingFists应运而生，其可被视为是面向AI领域的低代码应用框架。力图成为LainChain的可视化平替，能够全面支持非结构化数据的ETL处理以及包括LLM在内的各类模型的应用。同时，其也支持作业调度，可确保应用与生产环境的集成。HuggingFists与LainChain一样，目前也在持续完善中，真正做到LainChain的平替尚待时日。</p>
<p>下载Huggingfist</p>
<p>下载地址</p>
<p><a href="https://github.com/Datayoo/HuggingFists/blob/main/HuggingFists">https://github.com/Datayoo/HuggingFists/blob/main/HuggingFists</a></p>
<p><img alt="alt text" src="/docs/ali/image-50.png"></p>
<p>下载解压后</p>
<p><img alt="alt text" src="/docs/ali/image-51.png"></p>
<p>要求有docker环境</p>
<p><img alt="alt text" src="/docs/ali/image-52.png"></p>
<p>安装点击install.bat</p>
<p><img alt="alt text" src="/docs/ali/image-53.png"></p>
<p>然后可以通过浏览器访问http://localhost:38172，docker desktop也可以看到容器http://localhost:38172/datasource/DbSource</p>
<p><img alt="alt text" src="/docs/ali/image-54.png"></p>
<p>目的是构建下图
<img alt="alt text" src="/docs/ali/image-55.png"></p>
<p>批量化创建节点，最终构建类似如图的知识图谱</p>
<p><img alt="alt text" src="/docs/ali/image-56.png"></p>
<p>demo1:
创建一个关于汉武帝的知识图谱</p>
<p>准备数据集</p>
<p><img alt="alt text" src="/docs/ali/image-57.png"></p>
<p><img alt="alt text" src="/docs/ali/image-58.png"></p>
<p>上传数据</p>
<p><img alt="alt text" src="/docs/ali/image-59.png"></p>
<p><img alt="alt text" src="/docs/ali/image-60.png"></p>
<p><img alt="alt text" src="/docs/ali/image-61.png"></p>
<p>数据库账号设置</p>
<p><img alt="alt text" src="/docs/ali/image-62.png"></p>
<p><img alt="alt text" src="/docs/ali/image-63.png"></p>
<p>注意：如果是通过docker运行数据库，可参考以下操作</p>
<p>docker ps</p>
<p><img alt="alt text" src="/docs/ali/image-65.png"></p>
<p>docker logs a98ff54e0ada</p>
<p><img alt="alt text" src="/docs/ali/image-66.png"></p>
<p><img alt="alt text" src="/docs/ali/image-69.png"></p>
<p><img alt="alt text" src="/docs/ali/image-70.png"></p>
<p><img alt="alt text" src="/docs/ali/image-71.png"></p>
<p><img alt="alt text" src="/docs/ali/image-72.png"></p>
<p><img alt="alt text" src="/docs/ali/image-73.png"></p>
<p><img alt="alt text" src="/docs/ali/image-74.png">
创建表</p>
<p><img alt="alt text" src="/docs/ali/image-64.png"></p>
<p><img alt="alt text" src="/docs/ali/image-75.png"></p>
<p>具体放text里</p>
<p>创建字段</p>
<p>注意：</p>
<p><img alt="alt text" src="/docs/ali/image-76.png"></p>
<p>huggingfists在docker容器中启动，无法直接访问本地端口，需要借助host.docker.internal</p>
<p>目前缺少json扁平化算子</p>
<p><img alt="alt text" src="/docs/ali/image-77.png"></p>
<p>解决办法是在github上下载juggingfists的operater扩展</p>
<p>地址如下：</p>
<p><a href="https://github.com/Datayoo/Operators">https://github.com/Datayoo/Operators</a></p>
<p><img alt="alt text" src="/docs/ali/image-83.png"></p>
<p>下载代码之后</p>
<p><img alt="alt text" src="/docs/ali/image-84.png"></p>
<p>找到关于算子的两个文件</p>
<p>进入系统-&gt; 资源库-&gt;算子库-&gt;上传刚才下载的算子库</p>
<p><img alt="alt text" src="/docs/ali/image-85.png"></p>
<p>上传的算子库是zip压缩包，需要先上传descriptor 的zip包，再上传oyez.zip包</p>
<p>neo4j数据库也是本地安装的二进制文件，所有访问方法与ollama相似</p>
<p><img alt="alt text" src="/docs/ali/image-86.png"></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-224ecc6e4d20e59f8fc1ec4333097cd9">6 - 6_关于LangChain的调研</h1>
    
	<p>一、什么是LangChain？
LangChain是一个强大的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由大型语言模型 (LLM) 和聊天模型提供支持的应用程序的过程。LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源，例如 API 和数据库。</p>
<p>官方文档：https://python.langchain.com/en/latest/</p>
<p>中文文档：https://www.langchain.com.cn/</p>
<p>LangChain本身并不开发LLMs，它的核心理念是为各种LLMs提供通用的接口，降低开发者的学习成本，方便开发者快速地开发复杂的LLMs应用。</p>
<p>官方的定义：LangChain是一个基于语言模型开发应用程序的框架。它可以实现以下应用程序：</p>
<p>数据感知：将语言模型连接到其他数据源
自主性：允许语言模型与其环境进行交互
主要价值在于：</p>
<p>组件化：为使用语言模型提供抽象层，以及每个抽象层的一组实现。组件是模块化且易于使用的，无论您是否使用LangChain框架的其余部分。
现成的链：结构化的组件集合，用于完成特定的高级任务
现成的链使得入门变得容易。对于更复杂的应用程序和微妙的用例，组件化使得定制现有链或构建新链变得更容易。</p>
<p>要使用 LangChain，开发人员首先要导入必要的组件和工具，例如 LLMs, chat models, agents, chains, 内存功能。这些组件组合起来创建一个可以理解、处理和响应用户输入的应用程序。</p>
<p>LangChain 为特定用例提供了多种组件，例如个人助理、文档问答、聊天机器人、查询表格数据、与 API 交互、提取、评估和汇总。</p>
<p>二、模型分类和特点
LangChain model 是一种抽象，表示框架中使用的不同类型的模型。</p>
<p>LangChain 中的模型分类：</p>
<p>LLM（大型语言模型）：这些模型将文本字符串作为输入并返回文本字符串作为输出。它们是许多语言模型应用程序的支柱。
聊天模型( Chat Model)：聊天模型由语言模型支持，但具有更结构化的 API。他们将聊天消息列表作为输入并返回聊天消息。这使得管理对话历史记录和维护上下文变得容易。
文本嵌入模型(Text Embedding Models)：这些模型将文本作为输入并返回表示文本嵌入的浮点列表。这些嵌入可用于文档检索、聚类和相似性比较等任务。</p>
<p>LangChain 的特点：</p>
<p>LLM 和提示：LangChain 使管理提示、优化它们以及为所有 LLM 创建通用界面变得容易。此外，它还包括一些用于处理 LLM 的便捷实用程序。</p>
<p>链(Chain)：这些是对 LLM 或其他实用程序的调用序列。LangChain 为链提供标准接口，与各种工具集成，为流行应用提供端到端的链。</p>
<p>数据增强生成：LangChain 使链能够与外部数据源交互以收集生成步骤的数据。例如，它可以帮助总结长文本或使用特定数据源回答问题。</p>
<p>Agents：Agents 让 LLM 做出有关行动的决定，采取这些行动，检查结果，并继续前进直到工作完成。LangChain 提供了代理的标准接口，多种代理可供选择，以及端到端的代理示例。
内存：LangChain 有一个标准的内存接口，有助于维护链或代理调用之间的状态。它还提供了一系列内存实现和使用内存的链或代理的示例。</p>
<p>评估：很难用传统指标评估生成模型。这就是为什么 LangChain 提供提示和链来帮助开发者自己使用 LLM 评估他们的模型。</p>
<p>三、关注了该技术的局限性</p>
<p>下面是一篇以&quot;为什么都放弃了LangChain？&ldquo;为题目的文章</p>
<p><a href="https://baijiahao.baidu.com/s?id=1802732876402932399&wfr=spider&for=pc">https://baijiahao.baidu.com/s?id=1802732876402932399&wfr=spider&for=pc</a></p>
<p><img alt="alt text" src="/docs/ali/image-68.png"></p>
<p>关于LangChain推荐这个中文官方文档，如下</p>
<p><a href="https://js.langchain.com.cn/docs/getting-started/guide-llm">https://js.langchain.com.cn/docs/getting-started/guide-llm</a></p>
<p><img alt="alt text" src="/docs/ali/image-67.png"></p>
<p>介绍了LangChain的基本使用方法，包括安装入门，基本语法，以及API调用等</p>
<p>正在基于LangChain进行一个简单的demo项目
目标是
<a href="https://www.bilibili.com/video/BV1ng4y1U7bH/">https://www.bilibili.com/video/BV1ng4y1U7bH/</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d3734564d8e86d400fa1658422ed585b">7 - 7_基于Langchain和ollama构建的本地RAG雏形</h1>
    
	<p>效果展示</p>
<p>首先用ai虚构了一个修仙短篇故事，以沁三郎为主人公，写一个凡人修仙的故事</p>
<p><img alt="alt text" src="/docs/ali/image-90.png"></p>
<p>然后把这个故事存成san.txt文件</p>
<p><img alt="alt text" src="/docs/ali/image-89.png"></p>
<p>测试1：测试模型是否能根据提供的文本内容回答问题
向模型提问：&ldquo;沁三郎住哪里？&rdquo;</p>
<p><img alt="alt text" src="/docs/ali/image-88.png"></p>
<p>回答：&ldquo;沁三郎居住在遥远东方的一片名为青云大陆的地方，这里山川秀美，灵气充沛，是修炼者梦寐以求的圣地。&rdquo;</p>
<p><img alt="alt text" src="/docs/ali/image-91.png"></p>
<p>这和提供的文本内容基本一致</p>
<p>测试2：测试模型是否能根据提供的文本内容综合回答问题
向模型提问：&ldquo;沁三郎修炼的功法是什么？&rdquo;</p>
<p><img alt="alt text" src="/docs/ali/image-92.png"></p>
<p>回答： &ldquo;《灵元诀》和《青云诀》是沁三郎修炼的功法。&rdquo;</p>
<p><img alt="alt text" src="/docs/ali/image-93.png"></p>
<p><img alt="alt text" src="/docs/ali/image-94.png"></p>
<p>这和提供的文本内容基本一致</p>
<p>测试3：测试模型是否能回答文本之外的问题</p>
<p>向模型提问：&ldquo;沁三郎喜欢肯德基还是蜜雪冰城&rdquo;</p>
<p><img alt="alt text" src="/docs/ali/image-95.png"></p>
<p>回答： &ldquo;沁三郎对修仙之道充满了好奇和热情，因此他的饮食习惯可能与修炼者的日常饮食有所差异，但无法确定他是否特别喜爱肯德基或蜜雪冰城。在《凡人成仙录》的故事情节中，沁三郎更专注于自己的修行之路，并没有提及他对食品的选择偏好。&rdquo;</p>
<p>这体现了模型对文本之外问题的理解能力</p>
<p>下面是这个demo的完整代码</p>
<pre tabindex="0"><code>from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

# 1. 文档加载与文本分割
def load_and_split_documents(file_path: str):
    loader = TextLoader(file_path, encoding=&#34;utf-8&#34;)
    documents = loader.load()
    
    # 使用文本分割器进行分割
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)
    
    return split_docs

# 2. 向量化存储
def create_vectorstore(documents):
    embeddings = HuggingFaceEmbeddings(model_name=&#34;sentence-transformers/all-mpnet-base-v2&#34;)
    vectorstore = FAISS.from_documents(documents, embeddings)
    return vectorstore

# 3. 向量库检索
def retrieve_documents(query: str, retriever):
    docs = retriever.get_relevant_documents(query)
    return docs

# 4. 生成问答
def ask_question(question: str, qa_chain):
    response = qa_chain.run(question)
    return response

# 5. FastAPI 接口定义
app = FastAPI()

class Query(BaseModel):
    question: str

@app.post(&#34;/ask&#34;)
def ask(query: Query):
    answer = ask_question(query.question, qa_chain)
    return {&#34;answer&#34;: answer}

if __name__ == &#34;__main__&#34;:
    # 加载并处理文档
    documents = load_and_split_documents(r&#39;C:\Users\ChangjianPan\OneDrive - Kongsberg Digital AS\Desktop\三创\ai-backend\backend\san.txt&#39;)

    # 创建向量存储
    vectorstore = create_vectorstore(documents)

    # 构建检索器
    retriever = vectorstore.as_retriever()

    # 使用Ollama模型创建问答链，连接到本地Ollama服务
    llm = Ollama(model=&#34;qwen2:1.5b&#34;)
    qa_chain = RetrievalQA.from_chain_type(llm, chain_type=&#34;stuff&#34;, retriever=retriever)

    # 启动FastAPI服务
    uvicorn.run(app, host=&#34;0.0.0.0&#34;, port=8002)
</code></pre><p>环境准备
确保安装以下所需的Python库：</p>
<pre tabindex="0"><code>pip install langchain ollama transformers faiss-cpu fastapi uvicorn

pip install -U langchain-community

pip install -U langchain-huggingface

pip install sentence-transformers
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-750249da55139a8c5fa2e11520ded12a">8 - 8_基于ollama对RAG的调用来构建neo4j的知识图谱</h1>
    
	<p>基本配置：</p>
<p>olllama+qwen2:1.5b</p>
<p>ollama serve 查看ollama的启动情况</p>
<p><img alt="alt text" src="/docs/ali/image-96.png"></p>
<p>当前为未连接状态，需要连接后才能使用</p>
<p>ollama list 查看已经安装好的本地模型</p>
<p><img alt="alt text" src="/docs/ali/image-97.png"></p>
<p>ollama run qwen2:1.5b  启动模型</p>
<p><img alt="alt text" src="/docs/ali/image-98.png"></p>
<p>ollama ps 查看正在运行的模型</p>
<p><img alt="alt text" src="/docs/ali/image-99.png"></p>
<p>neo4j的启动</p>
<p>为了方便可以直接配置环境变量</p>
<p><img alt="alt text" src="/docs/ali/image-100.png"></p>
<p>然后就运行cmd</p>
<p>neo4j.bat console</p>
<p><img alt="alt text" src="/docs/ali/image-101.png"></p>
<p>出现如图内容后，说明启动成功</p>
<p>进入</p>
<p>http://localhost:7474/</p>
<p>输入账号，密码</p>
<p>到达界面</p>
<p><img alt="alt text" src="/docs/ali/image-102.png"></p>
<p>准备数据材料</p>
<p><img alt="alt text" src="/docs/ali/image-103.png"></p>
<p>启动py文件</p>
<p><img alt="alt text" src="/docs/ali/image-104.png"></p>
<p>运行结果</p>
<p><img alt="alt text" src="/docs/ali/image-105.png"></p>
<p>在neo4j数据库查询</p>
<p>MATCH (n)</p>
<p>RETURN n</p>
<p><img alt="alt text" src="/docs/ali/image-106.png"></p>
<p>代码如下</p>
<pre tabindex="0"><code>import json
import re
import os
import jieba
from langchain_community.llms import Ollama
from neo4j import GraphDatabase

# Neo4j连接配置
NEO4J_URI = &#34;bolt://localhost:7687&#34;
NEO4J_USER = &#34;neo4j&#34;
NEO4J_PASSWORD = &#34;123456Aa&#34;

# Ollama API配置
OLLAMA_MODEL = &#34;qwen2:1.5b&#34;

# 文件夹路径
FOLDER_PATH = r&#34;C:\Users\ChangjianPan\OneDrive - Kongsberg Digital AS\Desktop\三创\ai-backend\backend\mysite\chat\ragkm&#34;

# 1. 读取文件内容
def read_file(file_path):
    try:
        with open(file_path, &#39;r&#39;, encoding=&#39;utf-8&#39;) as file:
            text = file.read()
        return text
    except Exception as e:
        print(f&#34;Error reading file {file_path}: {e}&#34;)
        return &#34;&#34;

# 2. 文本预处理
def preprocess_text(text):
    text = text.strip()
    text = re.sub(r&#39;\s+&#39;, &#39; &#39;, text)
    segmented_text = &#34; &#34;.join(jieba.cut(text))
    paragraphs = segmented_text.split(&#39;\n\n&#39;)
    return paragraphs

def extract_entities_and_relations(text):
    llm = Ollama(model=OLLAMA_MODEL)
    prompt = (
        &#34;Analyze the following text and extract detailed information in JSON format:\n&#34;
        &#34;1. Characters: Extract all characters&#39; names, titles, and honorifics found in the text.\n&#34;
        &#34;2. Relationships: Extract all relationships between characters, including the nature of the relationship.\n&#34;
        &#34;3. Events: Extract all significant events related to the characters mentioned in the text.\n&#34;
        &#34;Format the output as a JSON object with keys &#39;entities&#39;, &#39;relationships&#39;, and &#39;events&#39;.\n&#34;
        &#34;Example:\n&#34;
        &#34;{\n&#34;
        &#34;  &#39;entities&#39;: [\n&#34;
        &#34;    {&#39;name&#39;: &#39;Emperor X&#39;, &#39;title&#39;: &#39;Emperor&#39; },\n&#34;
        &#34;    {&#39;name&#39;: &#39;General Y&#39;, &#39;title&#39;: &#39;General&#39;},\n&#34;
        &#34;    ...\n&#34;
        &#34;  ],\n&#34;
        &#34;  &#39;relationships&#39;: [\n&#34;
        &#34;    {&#39;entity1&#39;: &#39;Emperor X&#39;, &#39;relation&#39;: &#39;Lead&#39;, &#39;entity2&#39;: &#39;Battle of Y&#39;},\n&#34;
        &#34;    {&#39;entity1&#39;: &#39;General Y&#39;, &#39;relation&#39;: &#39;Fought in&#39;, &#39;entity2&#39;: &#39;Battle of Y&#39;},\n&#34;
        &#34;    ...\n&#34;
        &#34;  ],\n&#34;
        &#34;  &#39;events&#39;: [\n&#34;
        &#34;    {&#39;event_name&#39;: &#39;Battle of Y&#39;, &#39;event_date&#39;: &#39;Year X&#39;, &#39;related_to&#39;: &#39;Emperor X&#39;},\n&#34;
        &#34;    ...\n&#34;
        &#34;  ]\n&#34;
        &#34;}\n\n&#34;
        f&#34;{text}&#34;
    )
    
    try:
        result = llm.invoke(prompt)
        print(&#34;Ollama API response:&#34;, result)  # Print the raw response to inspect it
        
        if not result:
            print(&#34;Received empty response from Ollama&#34;)
            return {&#34;entities&#34;: [], &#34;relationships&#34;: [], &#34;events&#34;: []}
        
        # Check if the response starts with &#39;{&#39; indicating JSON
        if result.strip().startswith(&#39;{&#39;):
            extracted_data = json.loads(result)
            if not isinstance(extracted_data, dict):
                raise ValueError(&#34;Invalid JSON format: Expected a JSON object&#34;)
            return extracted_data
        else:
            print(&#34;Unexpected response format from Ollama&#34;)
            return {&#34;entities&#34;: [], &#34;relationships&#34;: [], &#34;events&#34;: []}
    
    except json.JSONDecodeError as e:
        print(f&#34;Error decoding JSON: {e}&#34;)
        return {&#34;entities&#34;: [], &#34;relationships&#34;: [], &#34;events&#34;: []}
    except Exception as e:
        print(f&#34;Error during Ollama invocation: {e}&#34;)
        return {&#34;entities&#34;: [], &#34;relationships&#34;: [], &#34;events&#34;: []}



# 4. 将实体、事件和关系导入Neo4j
def create_graph(tx, extracted_data):
    for entity in extracted_data[&#39;entities&#39;]:
        name = entity.get(&#39;name&#39;)
        title = entity.get(&#39;title&#39;, &#39;Unknown&#39;)
        honorific = entity.get(&#39;honorific&#39;, &#39;Unknown&#39;)
        if name:
            tx.run(&#34;MERGE (e:Entity {name: $name}) &#34;
                   &#34;SET e.title = $title, e.honorific = $honorific&#34;,
                   name=name, title=title, honorific=honorific)
    
    for event in extracted_data[&#39;events&#39;]:
        event_name = event.get(&#39;event_name&#39;)
        event_date = event.get(&#39;event_date&#39;, &#39;Unknown&#39;)
        related_to = event.get(&#39;related_to&#39;, &#39;Unknown&#39;)
        if event_name:
            tx.run(&#34;MERGE (e:Event {name: $name}) &#34;
                   &#34;SET e.date = $date&#34;,
                   name=event_name, date=event_date)
            if related_to:
                tx.run(&#34;MATCH (e:Event {name: $event_name}) &#34;
                       &#34;MATCH (p:Entity {name: $related_to}) &#34;
                       &#34;MERGE (p)-[:INVOLVED_IN]-&gt;(e)&#34;,
                       event_name=event_name, related_to=related_to)
    
    for relation in extracted_data[&#39;relationships&#39;]:
        entity1 = relation.get(&#39;entity1&#39;)
        entity2 = relation.get(&#39;entity2&#39;)
        rel_type = relation.get(&#39;relation&#39;, &#39;Unknown&#39;)
        if entity1 and entity2:
            tx.run(&#34;MERGE (e1:Entity {name: $entity1}) &#34;
                   &#34;MERGE (e2:Entity {name: $entity2}) &#34;
                   &#34;MERGE (e1)-[:RELATION {type: $relation}]-&gt;(e2)&#34;,
                   entity1=entity1, entity2=entity2, relation=rel_type)

def import_to_neo4j(extracted_data):
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    with driver.session() as session:
        try:
            session.execute_write(create_graph, extracted_data)
        except Exception as e:
            print(f&#34;Error during Neo4j operation: {e}&#34;)
    driver.close()

# 主流程
def main():
    all_extracted_data = {&#34;entities&#34;: [], &#34;relationships&#34;: [], &#34;events&#34;: []}
    
    for file_name in os.listdir(FOLDER_PATH):
        file_path = os.path.join(FOLDER_PATH, file_name)
        
        if os.path.isfile(file_path):
            text = read_file(file_path)
            if text:
                paragraphs = preprocess_text(text)
                for paragraph in paragraphs:
                    extracted_data = extract_entities_and_relations(paragraph)
                    all_extracted_data[&#39;entities&#39;].extend(extracted_data[&#39;entities&#39;])
                    all_extracted_data[&#39;relationships&#39;].extend(extracted_data[&#39;relationships&#39;])
                    all_extracted_data[&#39;events&#39;].extend(extracted_data[&#39;events&#39;])
    
    if all_extracted_data:
        import_to_neo4j(all_extracted_data)
        print(&#34;知识图谱构建完成！&#34;)
    else:
        print(&#34;没有提取到实体和关系。&#34;)

if __name__ == &#34;__main__&#34;:
    main()
</code></pre>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-35862ca271f0c20c95171af619a3df1b">9 - 9_Huggingface transformers文本生成</h1>
    
	<p>文章地址</p>
<p><a href="https://fancyerii.github.io/2023/12/19/hg-transformer-generate/">https://fancyerii.github.io/2023/12/19/hg-transformer-generate/</a></p>
<p>本文整理了Huggingface transformers文本生成相关的资料。</p>
<p>目录</p>
<p>文本生成策略
默认文本生成策略
定制文本生成
保存自定义解码策略与您的模型
Streaming
解码策略
贪婪搜索(Greedy Search)
对比搜索(Contrastive search)
多项式采样(Multinomial sampling)
Beam搜索解码(Beam-search decoding)
Beam搜索多项式采样(Beam-search multinomial sampling)
多样性beam搜索(Diverse beam search decoding)
辅助解码(Assisted Decoding)
相关代码</p>
<ol>
<li>约束的Beam搜索(Constrained Beam Search)</li>
<li>贪婪搜索</li>
<li>对比搜索</li>
<li>多项式采样</li>
<li>Beam搜索</li>
<li>多样性Beam搜索(Group Beam Search)</li>
<li>多项式采样Beam搜索</li>
<li>辅助搜索
8.1 语言模型解码回顾
8.2 使用辅助搜索的贪心解码算法
参考资料
文本生成策略
文本生成对于许多自然语言处理（NLP）任务至关重要，例如开放式文本生成、摘要、翻译等。它还在许多混合模态应用中发挥作用，这些应用以文本作为输出，如语音转文字和视觉转文字。一些能够生成文本的模型包括GPT2、XLNet、OpenAI GPT、CTRL、TransformerXL、XLM、Bart、T5、GIT、Whisper。</li>
</ol>
<p>请注意，generate方法的输入取决于模型的模态性。这些输入由模型的预处理器类（如AutoTokenizer或AutoProcessor）返回。如果模型的预处理器创建了多种类型的输入，请将所有输入传递给generate()。您可以在相应模型的文档中了解有关各个模型预处理器的更多信息。</p>
<p>选择生成文本的输出token的过程称为解码，您可以自定义generate()方法将使用的解码策略。修改解码策略不会更改任何可训练参数的值。然而，它可能会显著影响生成输出的质量，有助于减少文本中的重复并使其更连贯。</p>
<p>默认文本生成策略
模型的解码策略是在其生成配置中定义的。在使用预训练模型用pipeline进行生成时，模型调用PreTrainedModel.generate()方法，在幕后应用默认的生成配置。当没有使用自定义配置保存模型时，也会使用默认配置。</p>
<p>当您显式加载一个模型时，您可以通过model.generation_config检查生成配置：</p>
<p>from transformers import AutoModelForCausalLM</p>
<p>model = AutoModelForCausalLM.from_pretrained(&ldquo;distilgpt2&rdquo;)
model.generation_config
输出：
GenerationConfig {
&ldquo;bos_token_id&rdquo;: 50256,
&ldquo;eos_token_id&rdquo;: 50256,
}
打印model.generation_config只显示与默认生成配置不同的值，而不列出任何默认值。</p>
<p>默认生成配置新生成的最大token为20，以避免遇到资源限制【这对大部分应用来说显然不够】。默认解码策略是贪婪搜索，这是一种最简单的解码策略，选择具有最高概率的token作为下一个token。对于许多任务和小的输出尺寸，这种方法效果良好。然而，当用于生成较长的输出时，贪婪搜索可能开始产生高度重复的结果。</p>
<p>定制文本生成
您可以通过直接将参数及其值传递给generate方法来覆盖任何generation_config：</p>
<p>my_model.generate(**inputs, num_beams=4, do_sample=True)
即使默认的解码策略对您的任务大多数情况下有效，您仍然可以微调一些参数。一些常常调整的参数包括：</p>
<p>max_new_tokens：生成的最大token数。换句话说，输出序列的大小，不包括提示中的token。作为使用输出长度作为停止标准的替代方法，您可以选择在完整生成超过一定时间量时停止生成。要了解更多信息，请查看StoppingCriteria。</p>
<p>num_beams：通过指定高于1的beam数，您实际上是从贪婪搜索切换到beam搜索。此策略在每个时间步评估多个假设，最终选择具有整个序列最高概率的假设。这有利于识别以较低概率初始token开头的高概率序列，而这些序列在贪婪搜索中可能会被忽略。</p>
<p>do_sample：如果设置为True，此参数启用解码策略，如多项式采样、beam搜索多项式采样、Top-K采样和Top-p采样。所有这些策略都从整个词汇表的概率分布中选择下一个token，具有各种特定于策略的调整。</p>
<p>num_return_sequences：每个输入返回的序列候选数。此选项仅适用于支持多个序列候选的解码策略，例如beam搜索和采样的变体。像贪婪搜索和对比搜索这样的解码策略返回单个输出序列。</p>
<p>保存自定义解码策略与您的模型
如果训练了自己的模型，并且想把默认的生成配置保存下来，那么可以执行以下步骤：</p>
<p>创建一个GenerationConfig类实例。
指定解码策略参数。
使用GenerationConfig.save_pretrained()保存您的生成配置，确保将其config_file_name参数留空。
将push_to_hub设置为True，以将您的配置上传到模型的仓库。
from transformers import AutoModelForCausalLM, GenerationConfig</p>
<p>model = AutoModelForCausalLM.from_pretrained(&ldquo;my_account/my_model&rdquo;)
generation_config = GenerationConfig(
max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
)
generation_config.save_pretrained(&ldquo;my_account/my_model&rdquo;, push_to_hub=True)
您还可以将多个生成配置存储在单个目录中，利用GenerationConfig.save_pretrained()中的config_file_name参数。然后，您可以使用GenerationConfig.from_pretrained()实例化它们。如果您希望为单个模型存储多个生成配置（例如，一个用于采样的创造性文本生成，另一个用于带有beam搜索的摘要），这将非常有用。您必须具有适当的Hub权限才能将配置文件添加到模型。</p>
<p>from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig</p>
<p>tokenizer = AutoTokenizer.from_pretrained(&ldquo;t5-small&rdquo;)
model = AutoModelForSeq2SeqLM.from_pretrained(&ldquo;t5-small&rdquo;)</p>
<p>translation_generation_config = GenerationConfig(
num_beams=4,
early_stopping=True,
decoder_start_token_id=0,
eos_token_id=model.config.eos_token_id,
pad_token=model.config.pad_token_id,
)</p>
<h1 id="tip-add-push_to_hubtrue-to-push-to-the-hub-translation_generation_configsave_pretrainedtmp-translation_generation_configjson">Tip: add <code>push_to_hub=True</code> to push to the Hub translation_generation_config.save_pretrained(&quot;/tmp&quot;, &ldquo;translation_generation_config.json&rdquo;)</h1>
<h1 id="you-could-then-use-the-named-generation-config-file-to-parameterize-generation-generation_config--generationconfigfrom_pretrainedtmp-translation_generation_configjson">You could then use the named generation config file to parameterize generation generation_config = GenerationConfig.from_pretrained(&quot;/tmp&quot;, &ldquo;translation_generation_config.json&rdquo;)</h1>
<p>inputs = tokenizer(&ldquo;translate English to French: Configuration files are easy to use!&rdquo;, return_tensors=&ldquo;pt&rdquo;)
outputs = model.generate(**inputs, generation_config=generation_config)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
Streaming
generate() 支持流式传输，通过其streamer输入。streamer输入与具有以下方法的类的任何实例兼容：put() 和 end()。在内部，put() 用于推送新的token，而end() 用于标记文本生成的结束。</p>
<p>流式传输类的 API 仍在开发中，未来可能会发生更改。</p>
<p>在实践中，您可以为各种目的创建自己的流式传输类！我们还为您准备了基本的流式传输类供您使用。例如，您可以使用TextStreamer类将generate()的输出流式传输到您的屏幕，逐词显示：</p>
<p>from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer</p>
<p>tok = AutoTokenizer.from_pretrained(&ldquo;gpt2&rdquo;)
model = AutoModelForCausalLM.from_pretrained(&ldquo;gpt2&rdquo;)
inputs = tok([&ldquo;An increasing sequence: one,&rdquo;], return_tensors=&ldquo;pt&rdquo;)
streamer = TextStreamer(tok)</p>
<h1 id="despite-returning-the-usual-output-the-streamer-will-also-print-the-generated-text-to-stdout-_--modelgenerateinputs-streamerstreamer-max_new_tokens20">Despite returning the usual output, the streamer will also print the generated text to stdout. _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)</h1>
<p>解码策略
generate()参数的某些组合，最终会修改generation_config，可以用于启用特定的解码策略。如果您对这个概念不太了解，我们建议阅读这篇博客文章【后面会介绍这篇博客】，详细说明了常见解码策略的工作原理。</p>
<p>在这里，我们将展示一些控制解码策略的参数，并说明您如何使用它们。</p>
<p>贪婪搜索(Greedy Search)
默认情况下，generate 使用贪婪搜索解码，因此您无需传递任何参数来启用它。这意味着num_beams被设置为1，而do_sample被设置为False。</p>
<p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>prompt = &ldquo;I look forward to&rdquo;
checkpoint = &ldquo;distilgpt2&rdquo;</p>
<p>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>model = AutoModelForCausalLM.from_pretrained(checkpoint)
outputs = model.generate(**inputs)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
对比搜索(Contrastive search)
对比搜索解码策略是在2022年的论文《A Contrastive Framework for Neural Text Generation》中提出的。该策略在生成非重复但连贯的长文本输出方面表现出卓越的结果。要了解对比搜索的工作原理，请查阅这篇博客文章。启用和控制对比搜索行为的两个主要参数是penalty_alpha和top_k：</p>
<p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>
<p>checkpoint = &ldquo;gpt2-large&rdquo;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)</p>
<p>prompt = &ldquo;Hugging Face Company is&rdquo;
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
多项式采样(Multinomial sampling)
与总是选择具有最高概率的token作为下一个token的贪婪搜索相反，多项式采样根据模型给出的整个词汇表上的概率分布随机选择下一个token。具有非零概率的每个token都有被选择的机会，从而降低了重复的风险。</p>
<p>要启用多项式采样，请设置do_sample=True和num_beams=1。</p>
<p>from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
set_seed(0)  # For reproducibility
checkpoint = &ldquo;gpt2-large&rdquo;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)</p>
<p>prompt = &ldquo;Today was an amazing day because&rdquo;
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
Beam搜索解码(Beam-search decoding)
与贪婪搜索不同，beam搜索解码在每个时间步保留多个假设，最终选择整个序列中具有最高概率的假设。这有利于识别以较低概率初始token开头的高概率序列，而这些序列在贪婪搜索中可能会被忽略。</p>
<p>要启用此解码策略，请指定大于1的num_beams。</p>
<p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>prompt = &ldquo;It is astonishing how one can&rdquo;
checkpoint = &ldquo;gpt2-medium&rdquo;</p>
<p>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>model = AutoModelForCausalLM.from_pretrained(checkpoint)</p>
<p>outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
Beam搜索多项式采样(Beam-search multinomial sampling)
正如其名称所示，这种解码策略将beam搜索与多项式采样相结合。您需要指定大于1的num_beams，并设置do_sample=True以使用此解码策略。</p>
<p>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
set_seed(0)  # For reproducibility
prompt = &ldquo;translate English to German: The house is wonderful.&rdquo;
checkpoint = &ldquo;t5-small&rdquo;</p>
<p>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</p>
<p>outputs = model.generate(**inputs, num_beams=5, do_sample=True)
tokenizer.decode(outputs[0], skip_special_tokens=True)
多样性beam搜索(Diverse beam search decoding)
多样性beam搜索解码策略是beam搜索策略的扩展，允许生成一个更多样化的beam序列集合供选择。要了解其工作原理，请参阅《Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models》。该方法有三个主要参数：num_beams、num_beam_groups和diversity_penalty。多样性惩罚确保在组间输出是不同的，而在每个组内使用beam搜索。</p>
<p>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</p>
<p>checkpoint = &ldquo;google/pegasus-xsum&rdquo;
prompt = (
&ldquo;The Permaculture Design Principles are a set of universal design principles &quot;
&ldquo;that can be applied to any location, climate and culture, and they allow us to design &quot;
&ldquo;the most efficient and sustainable human habitation and food production systems. &quot;
&ldquo;Permaculture is a design system that encompasses a wide variety of disciplines, such &quot;
&ldquo;as ecology, landscape design, environmental science and energy conservation, and the &quot;
&ldquo;Permaculture design principles are drawn from these various disciplines. Each individual &quot;
&ldquo;design principle itself embodies a complete conceptual framework based on sound &quot;
&ldquo;scientific principles. When we bring all these separate principles together, we can &quot;
&ldquo;create a design system that both looks at whole systems, the parts that these systems &quot;
&ldquo;consist of, and how those parts interact with each other to create a complex, dynamic, &quot;
&ldquo;living system. Each design principle serves as a tool that allows us to integrate all &quot;
&ldquo;the separate parts of a design, referred to as elements, into a functional, synergistic, &quot;
&ldquo;whole system, where the elements harmoniously interact and work together in the most &quot;
&ldquo;efficient way possible.&rdquo;
)</p>
<p>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</p>
<p>outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)
tokenizer.decode(outputs[0], skip_special_tokens=True)
本指南阐明了启用各种解码策略的主要参数。generate方法还存在更高级的参数，它们让您对generate方法的行为有更进一步的控制。有关所有可用参数的完整列表，请参阅API文档。</p>
<p>辅助解码(Assisted Decoding)
辅助解码是对上述解码策略的修改，它使用具有相同分词器的辅助模型模型（理想情况下是一个更小的模型）贪婪生成几个候选token。然后，主模型在单个前向传递中验证候选token，从而加速解码过程。目前，辅助解码仅支持贪婪搜索和采样，并且不支持批处理输入。要了解有关辅助解码的更多信息，请查阅此博客文章。</p>
<p>要启用辅助解码，请使用assistant_model参数设置一个模型。</p>
<p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>prompt = &ldquo;Alice and Bob&rdquo;
checkpoint = &ldquo;EleutherAI/pythia-1.4b-deduped&rdquo;
assistant_checkpoint = &ldquo;EleutherAI/pythia-160m-deduped&rdquo;</p>
<p>tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors=&ldquo;pt&rdquo;)</p>
<p>model = AutoModelForCausalLM.from_pretrained(checkpoint)
assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
outputs = model.generate(**inputs, assistant_model=assistant_model)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
相关代码
上面的解码策略选择，可以通过transformers/generation/utils.py里的代码来验证：</p>
<pre><code>def _get_generation_mode(
    self, generation_config: GenerationConfig, assistant_model: Optional[&quot;PreTrainedModel&quot;]
) -&gt; GenerationMode:
    &quot;&quot;&quot; Returns the generation mode triggered by a [`GenerationConfig`] instance. &quot;&quot;&quot;
    if generation_config.constraints is not None or generation_config.force_words_ids is not None:
        generation_mode = GenerationMode.CONSTRAINED_BEAM_SEARCH
    elif generation_config.num_beams == 1:
        if generation_config.do_sample is False:
            if (
                generation_config.top_k is not None
                and generation_config.top_k &gt; 1
                and generation_config.penalty_alpha is not None
                and generation_config.penalty_alpha &gt; 0
            ):
                generation_mode = GenerationMode.CONTRASTIVE_SEARCH
            else:
                generation_mode = GenerationMode.GREEDY_SEARCH
        else:
            generation_mode = GenerationMode.SAMPLE
    else:
        if generation_config.num_beam_groups &gt; 1:
            generation_mode = GenerationMode.GROUP_BEAM_SEARCH
        elif generation_config.do_sample is True:
            generation_mode = GenerationMode.BEAM_SAMPLE
        else:
            generation_mode = GenerationMode.BEAM_SEARCH

    # Assisted generation may extend some generation modes         if assistant_model is not None:
        if generation_mode in (&quot;greedy_search&quot;, &quot;sample&quot;):
            generation_mode = GenerationMode.ASSISTED_GENERATION
        else:
            raise ValueError(
                &quot;You've set `assistant_model`, which triggers assisted generate. Currently, assisted generate &quot;
                &quot;is only supported with Greedy Search and Sample.&quot;
            )
    return generation_mode
</code></pre>
<ol>
<li>
<p>约束的Beam搜索(Constrained Beam Search)
如果constraints或者force_words_ids非空，函数返回CONSTRAINED_BEAM_SEARCH。约束的Beam搜索可以强制解码的结果包含某些token或者更复杂的约束(如短语约束)，详细介绍可以参考Guiding Text Generation with Constrained Beam Search in 🤗 Transformers。</p>
</li>
<li>
<p>贪婪搜索
num_beams==1并且do_sample==False时执行这个算法。贪婪算法在每个时刻都选择概率(logit)最大的那个token，这是最简单的算法。</p>
</li>
</ol>
<p>比如上图，prompt是”The”，然后第一个时刻概率最大的是”nice”、”dog”和”car”，贪婪算法选择概率最大的nice。后面也是类似的。所以贪婪算法是一种确定性的算法。</p>
<ol start="3">
<li>对比搜索
penalty_alpha&gt;0并且top_k&gt;1时执行这个算法。这个算法详细介绍请参考Generating Human-level Text with Contrastive Search in Transformers，这里简单介绍一下它的思想。这个算法的解码算法如下式子所示：</li>
</ol>
<p>其中，V(k)
是语言模型概率分布pθ(v|x&lt;t)
的概率排在top-k的k个token。第一项，即模型置信度，是语言模型对候选 v 的预测概率。第二项，退化惩罚，表示了候选 v 相对于先前上下文x&lt;t
和函数s(⋅,⋅)
计算的余弦相似度。具体而言，退化惩罚定义为候选 v 的token的隐向量hv
与先前上下文 x&lt;t
中所有token的隐向量之间的最大余弦相似度。直观地说，候选 v 的较大退化惩罚意味着它在表示空间中更类似于上下文，因此更有可能导致模型退化问题。超参数α
调节这两个组成部分的重要性。当α=0
时，对比搜索变为基本的贪婪搜索。</p>
<ol start="4">
<li>多项式采样
num_beams==1并且do_sample==True时执行。每次根据softmax(logits)的多项式概率进行采样。为了防止概率特别低的词被采样，可以使用top_p或者top_k进行过滤。</li>
</ol>
<p>top_k比较简单，只保留概率最大的k个token，然后重新用softmax计算其概率。但是这有一个问题，因为top_k是固定的，有的时候，某些token其实概率很低，但是因一定要凑够k个，也会被留下来。与之相反，有的时候，由于概率分布很均匀，排在top_k+1的token也可能概率还不小，但是不能留下来。为了解决这个问题，top_p这个参数派上用场，比如top_p=0.92，那么只有某个token的概率是最大概率token的92%以上都会被保留下来。这两个参数可以同时使用，那么就是一种逻辑与的关系，也就是一个条件不满足就会被扔掉。</p>
<p>除了这两个参数，还有一个temperature参数，这个参数越小，那么采样越倾向于概率大的token。极端情况如果temperature趋近于零(这里不能为零，但是像vLLM可以设置为零)，它就等价于贪心算法。</p>
<ol start="5">
<li>Beam搜索
num_beams&gt;1并且do_sample=False。贪心算法的问题是有的时候前面某个词的概率挺高，但是后面就没有好的token了。就像人生，开局好不见得全局好。贪心算法会把一些全局较优但是一开始不好的路径淘汰掉，为了避免这个问题，Beam搜索会在每个时刻同时保留最优的num_beams条路径。当然你说一上来的排名倒数第一，后面逆袭成为第一，这种事情也不是没有，但是概率太小了，而且从计算的角度来说保留所有路径基本是不可能的。Beam搜索的一个示例如下图：</li>
</ol>
<p>这个示例beam为2，第一次选择时排在第2的”dog”也被保留下来。最后全局最优的是得分0.9的路径成为最终结果。我们可以看到，Beam搜索也是确定性的算法，它相比贪心算法更可能找到全局最优，但是由于每次保留和展开多条路径，因此其速度比贪心算法慢。num_beams越大，速度越慢，找到全局最优的可能性越大。</p>
<ol start="6">
<li>多样性Beam搜索(Group Beam Search)
`条件为num_beams&gt;1并且num_beam_groups&gt;1。多样性Beam搜索任务普通的Beam搜索虽然会保留多条路径，但是这些路径的相似度都很高(比如”the book is mine”把mine改成his。为了解决这个问题，多样性beam搜索把num_beams分成num_beam_groups个组，每个组num_beams/num_beam_groups条路径，每个组使用普通的beam搜索解码。但是从第二个组开始，除了用LLM计算生成token的概率，还会增加一个所谓的不相似项：</li>
</ol>
<p>这个公式用自然语言描述就是：它定义了一个序列y[t]
和第g组的所有序列Yg[t]
的不相似度。我们希望这个越大越好，这样就能有多样性。</p>
<p>因此第g组的beam搜索目标变成：</p>
<p>看起来这个公式很复杂，仔细分析其实它表达的就是：除了LLM的生成概率(第一项)，还需要考虑第二项那个sum求和，它计算当前解码序列ygb,[t]
和之前的1～g-1个组的不相似度。</p>
<p>我们看一个例子：</p>
<p>比如在第4个时刻，解码gruop3的时候，”several birds are”会避免之前group出现过的”birds, the , an”这些词。大概原理就是这样，更多细节请参考论文。使用的方法，除了参数num_beams和num_beam_groups，细心的读者可能会问怎么控制超参数λg
呢？这个参数越大，那么组之间的差异越大。另外不相似度是怎么定义的呢？Huggingface实现的是HammingDiversityLogitsProcessor，具体原理不介绍了。感兴趣的读者自己看论文和阅读代码吧。用法就是通过参数diversity_penalty控制：</p>
<p>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch</p>
<h1 id="initialize-the-model-and-tokenizer-tokenizer--autotokenizerfrom_pretrainedt5-base">Initialize the model and tokenizer tokenizer = AutoTokenizer.from_pretrained(&ldquo;t5-base&rdquo;)</h1>
<p>model = AutoModelForSeq2SeqLM.from_pretrained(&ldquo;t5-base&rdquo;)</p>
<h1 id="a-long-text-about-the-solar-system-text--">A long text about the solar system text = (</h1>
<pre><code>&quot;The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, &quot;
&quot;either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight &quot;
&quot;planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System &quot;
&quot;bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant &quot;
&quot;interstellar molecular cloud.&quot;
</code></pre>
<p>)
inputs = tokenizer(&ldquo;summarize: &quot; + text, return_tensors=&ldquo;pt&rdquo;)</p>
<h1 id="generate-diverse-summary-outputs_diverse--modelgenerate">Generate diverse summary outputs_diverse = model.generate(</h1>
<pre><code>**inputs,
num_beam_groups=2,
diversity_penalty=10.0,
max_length=100,
num_beams=4,
num_return_sequences=2,
</code></pre>
<p>)
summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)</p>
<h1 id="generate-non-diverse-summary-outputs_non_diverse--modelgenerate">Generate non-diverse summary outputs_non_diverse = model.generate(</h1>
<pre><code>**inputs,
max_length=100,
num_beams=4,
num_return_sequences=2,
</code></pre>
<p>)
summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)</p>
<h1 id="with-diversity_penalty-the-resulting-beams-are-much-more-diverse-printsummary_non_diverse">With <code>diversity_penalty</code>, the resulting beams are much more diverse print(summary_non_diverse)</h1>
<p>print(summaries_diverse)
参数diversity_penalty最终会传给transformers.HammingDiversityLogitsProcessor。</p>
<ol start="7">
<li>多项式采样Beam搜索
需要满足的条件是num_beams&gt;1和do_sample=True。这是ChatGPT和GPT-4使用的搜索方法。有人认为ChatGPT使用了多项式采样的Beam搜索，比如reddit: chatGPT uses beam search。我认为不太可能，原因有二。一是在论文中InstructGPT里提到过用的是top_p采样，reddit回帖中有人有提到过。第二个原因是beam搜索是无法实现流式(streaming)输出的，因为它每次保留多个结果，之前较优的结果可能到最后就不是最优了。很多实时语音识别系统也会有流式解码，但是我们仔细观察会发现最终结果的前缀可能并不是之前的部分结果。但是ChatGPT第一个字输出后是永远不会改变的。当然理论上也可能用beam搜索解码N个字，然后一次输出，然后基于它(前N个字不变)再进行beam搜索。不过我认为这种可能性不大。</li>
</ol>
<p>beam搜索最简单就是top_k，当然也可以用top_p。另外就是temperature参数，如果temperature趋近于0，则beam搜索接近于贪心算法。</p>
<ol start="8">
<li>辅助搜索
这个搜索和前面不一样，它不能单独运行，而是作为另外一个模型的辅助。详细原理参考Assisted Generation: a new direction toward low-latency text generation。这里只做简单的介绍。</li>
</ol>
<p>8.1 语言模型解码回顾
在文本生成过程中，典型的迭代包括模型接收最新生成的token作为输入，再加上所有其他先前输入的缓存内部计算，然后返回下一个token的logits。缓存用于避免冗余计算，从而加速前向传递，但这并非强制性（可以部分使用）【关于KV cache读者可以参考PagedAttention论文解读】。当禁用缓存时，输入包含到目前为止生成的所有token序列，输出包含与序列中所有位置对应的下一个token的logits！在位置N处的logits对应于如果输入包含前N个token，则下一个token的分布，忽略序列中的所有后续token。在贪婪解码的特定情况下，如果将生成的序列作为输入传递，并对生成的logits应用argmax运算符，您将获得生成的序列。</p>
<p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>
<p>tok = AutoTokenizer.from_pretrained(&ldquo;distilgpt2&rdquo;)
model = AutoModelForCausalLM.from_pretrained(&ldquo;distilgpt2&rdquo;)</p>
<p>inputs = tok([&ldquo;The&rdquo;], return_tensors=&ldquo;pt&rdquo;)
generated = model.generate(**inputs, do_sample=False, max_new_tokens=10)
forward_confirmation = model(generated).logits.argmax(-1)</p>
<h1 id="we-exclude-the-opposing-tips-from-each-sequence-the-forward-pass-returns--the-logits-for-the-next-token-so-it-is-shifted-by-one-position-printgenerated0-1tolist--forward_confirmation0--1tolist---true">We exclude the opposing tips from each sequence: the forward pass returns # the logits for the next token, so it is shifted by one position. print(generated[0, 1:].tolist() == forward_confirmation[0, :-1].tolist())  # True</h1>
<p>这意味着您可以将模型的前向传递用于不同的目的：除了提供一些token以预测下一个token之外，还可以将一个序列传递给模型，然后双重检查模型是否会生成相同的序列（或其中的一部分）。</p>
<p>8.2 使用辅助搜索的贪心解码算法
您希望辅助模型能够快速生成候选序列，同时尽可能准确。如果辅助模型的质量较差，您将付出使用辅助模型模型的成本，却几乎没有任何好处。另一方面，优化候选序列的质量可能意味着使用较慢的辅助模型，导致净减速。虽然我们不能为您自动选择辅助模型模型，但我们已经包含了一个额外的要求和一个启发式方法，以确保与辅助模型一起花费的时间得到控制。</p>
<p>首先，要求辅助模型必须具有与您的模型完全相同的分词器。如果没有这个要求，就必须添加昂贵的token解码和重新编码步骤。此外，这些额外的步骤必须在CPU上执行，这反过来可能需要慢速的设备间数据传输。快速使用辅助模型对于实现辅助生成的好处至关重要。</p>
<p>最后，启发式方法。到这一点，您可能已经注意到辅助生成与电影《盗梦空间》之间的相似之处——毕竟，您正在文本生成中运行文本生成。每个候选token都会有一个辅助模型模型的前向传递，而我们知道前向传递是昂贵的。虽然您无法预先知道辅助模型模型将正确获取的token数量，但您可以跟踪此信息并使用它来限制请求给辅助模型的候选token的数量 - 输出的某些部分比其他部分更容易预测。</p>
<p>算法的具体步骤为：</p>
<p>使用贪婪解码生成助手模型的一定数量的候选token，产生候选项。第一次调用辅助生成时，生成的候选token数量初始化为5。
使用我们的模型对候选项进行前向传递，获取logits。
使用token选择方法（对于贪婪搜索使用.argmax()，对于采样使用.multinomial()）从logits中获取next_tokens。
将next_tokens与候选token进行比较，并获取匹配token的数量。请记住，此比较必须按照从左到右的因果关系进行：在第一次不匹配后，所有候选项都无效。
使用匹配token的数量来切分数据，并丢弃与未确认的候选token相关的变量。实质上，在next_tokens中保留匹配token以及第一个不同的token（我们的模型从有效的候选子序列中生成）。
调整下一次迭代中要生成的候选token的数量。我们的原始启发式方法，如果所有token都匹配，则增加2，否则减少1。
我们可以看一下一个示例，如果播放有问题可以另存为下载观看，下载地址。</p>
<p>在上面的例子，比如prompt是”The quick brown”，我们首先让辅助模型用贪心算法生成5个token，假设它生成的是”fox jumps into the”。然后我们把它们拼起来变成”The quick brown fox jumps into the”给大的生成模型，它的预测是”fox jumps over a”。我们发现两个模型的预测相同的前缀是”fox jumps”，到第3个token就不相同了。我们当然更信任大模型的结果，因此预测是”The quick brown fox jumps over”。接着把这个序列再交给辅助模型。循环上面的过程直到生成结束。当然这个过程可能会调节候选token的数量，比如使用简单的启发：如果所有token都匹配，加2，否则减少1，这里的例子就是减1变成4，也就是说下一次把”The quick brown fox jumps over”给辅助模型，它只需要输出4个token。</p>
<p>我们分析一下，最好的情况是两个模型的预测完全一样，这个时候只需要大的模型进行一次forward就能生成5个token(而原来只能生成一个，当然小模型还得forward 5次，不过我们一般选择的小模型速度比大模型快一个数量级)。最坏的情况呢？两个模型从第一个token开始预测结果就不相同，那么大模型forward一次输出5个token，只有一个是有用的(因为第2到第5个token的输入不是自回归产生，而是小模型给的，因为第一个就不能用，所以后面的计算都作废了)。</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a17fb2f37c71e00be1119a67b908c897">10 - 10_x</h1>
    
	<p>继续调研部分分为理论调研和代码调研</p>
<p>代码调研</p>
<p><a href="https://github.com/docker/genai-stack">https://github.com/docker/genai-stack</a></p>
<p>该项目具有以下调研的优势</p>
<ol>
<li>代码结构清晰，py文件用docker打包，和静态前端界面，FastAPI提供http请求,启动用sh脚本启动。</li>
<li>使用本地ollama作为模型，neo4j作为数据库，向量化存储，最终实现RAG+知识图谱上下文的综合搜素。</li>
<li>provide summarized answers with sources
提供带有来源的摘要答案</li>
<li>demonstrate difference between
展示两者之间的差异
RAG Disabled (pure LLM response)
RAG 禁用（纯LLM响应）
RAG Enabled (vector + knowledge graph context)
RAG 已启用（向量 + 知识图谱上下文）</li>
<li>PDF文件作为数据来源</li>
</ol>
<p>代码调研</p>

</div>



    
	
  



          </main>
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        
      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        
      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.js"></script>
<script defer src="/js/click-to-copy.js" crossorigin="anonymous"></script>
<script src='/js/tabpane-persist.js'></script>

  </body>
</html>
