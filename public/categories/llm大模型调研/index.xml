<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM大模型调研 on Goldydocs</title>
    <link>http://localhost:1313/categories/llm%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94/</link>
    <description>Recent content in LLM大模型调研 on Goldydocs</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 16 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/llm%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1_入手工具</title>
      <link>http://localhost:1313/docs/ali/1_%E5%85%A5%E6%89%8B%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/1_%E5%85%A5%E6%89%8B%E5%B7%A5%E5%85%B7/</guid>
      <description>一、工具下载地址 https://huggingface.co/models&#xA;Huggingface被称为大模型界的github, 在这里用户可以分享和下载预训练各种开源大模型。这让用户无需重新训练模型就能使用最先进的自然语言处理模型。&#xA;https://lmstudio.ai/&#xA;LM Studio支持多种模型的部署和微调，支持各种API调用</description>
    </item>
    <item>
      <title>2_构建本地大模型</title>
      <link>http://localhost:1313/docs/ali/2_%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/2_%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B/</guid>
      <description>下载 LM Studio 构建本地大模型&#xA;下载gguf模型&#xA;法一：在App直接下载模型（本质是在Huggingface官网搜索下载，所以需要外网VPN环境&#xA;法二：魔搭下载gguf模型（不需要vpn环境下载模型的方法）&#xA;https://www.modelscope.cn/&#xA;配置大模型&#xA;gguf文件的存储文件夹格式是models\Publisher\Repository，同时设置本地模型文件夹是models。&#xA;本地API服务 postman测试（测试json存放在md文件夹下）&#xA;python调用&#xA;messages中的content为返回值，通过completion.choices[0].message.content输出得到大模型返回内容</description>
    </item>
    <item>
      <title>3_ollama大模型构建</title>
      <link>http://localhost:1313/docs/ali/3_ollama%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/3_ollama%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA/</guid>
      <description>一、LLama3 大模型本地部署安装&#xA;首先去mata官网下载ollama客户端&#xA;Ollama&#xA;https://ollama.com/download&#xA;下载完成后，双击文件进行安装。安装成功后不会有反应。 打开cmd控制台，键入ollama，如果如下图显示命令提示就说明安装已经成功&#xA;进入官网找到想要下载的模型名称&#xA;https://ollama.com/library/llama3/tags。&#xA;如果使用的是普通电脑，建议选择8b（80亿参数）&#xA;查看自己电脑GPU&#xA;Windows设置 ——&amp;raquo;&amp;raquo;&amp;gt; 更新和安全 ——&amp;raquo;&amp;raquo;&amp;gt;&#xA;控制台中键入ollama run llama3:8b，会自动进行8B模型下载，该模型有80亿参数，普通电脑可以执行。如果想要70B可以修改为入ollama run llama3:8b。按回车键就开始下载并自动安装&#xA;当提示success时，说明已经成功安装了大模型。&#xA;接下来是可视化界面的安装&#xA;首先确保系统中已经安装了nodejs和git，如果没有可以自己去官网下载安装 https://nodejs.org/en&#xA;打开cmd控制台，键入node -v，如果显示vxx.xx.x.就说明安装成功了。&#xA;在cmd控制台中键入&#xA;git clone https://github.com/ollama-webui/ollama-webui-lite.git&#xA;添加资源镜像 在cmd控制台键入mkdir llama在当前路径下创建一个叫llama的文件夹，然后键入cd llama进入该文件夹下 在cmd控制台键入&#xA;git clone https://github.com/ollama-webui/ollama-webui-lite.git&#xA;将web ui界面下载回来 在cmd控制台键入&#xA;cd ollama-webui-lite&#xA;进入web ui项目文件夹 在cmd控制台键入&#xA;npm install&#xA;安装环境与依赖 在cmd控制台键入&#xA;npm run dev&#xA;启动web ui系统&#xA;然后点击Select a model 设置模型，我们选中已经下载好的8b模型&#xA;现在就可以在下方输入框进行输入对话了</description>
    </item>
    <item>
      <title>4_基于AnythingLLM构建RAG</title>
      <link>http://localhost:1313/docs/ali/4_%E5%9F%BA%E4%BA%8Eanythingllm%E6%9E%84%E5%BB%BArag/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/4_%E5%9F%BA%E4%BA%8Eanythingllm%E6%9E%84%E5%BB%BArag/</guid>
      <description>下载地址 https://anythingllm.com/&#xA;选择LLM模型（必须是本地有的模型） 选择向量数据库（默认） 选择嵌入引擎（默认） 1.创建一个专用工作区&#xA;有了不同的工作区，提供不同的数据，可以分别去做不同的角色工作&#xA;2.上传数据 包括pdf、word、excel等多种格式 也可以使用链接上传 嵌入向量数据库 3.查看效果 支持权限管理 支持API调用 </description>
    </item>
    <item>
      <title>5_基于Huggingdist和Neo4j构建RAG</title>
      <link>http://localhost:1313/docs/ali/5_%E5%9F%BA%E4%BA%8Ehuggingdist%E5%92%8Cneo4j%E6%9E%84%E5%BB%BArag/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/5_%E5%9F%BA%E4%BA%8Ehuggingdist%E5%92%8Cneo4j%E6%9E%84%E5%BB%BArag/</guid>
      <description>Neo4j的关键概念和特点 •Neo4j是一个开源的NoSQL图形存储数据库，可为应用程序提供支持ACID的后端。Neo4j的开发始于2003年，自2007年转变为开源图形数据库模型。程序员使用的是路由器和关系的灵活网络结构，而不是静态表，但是可以享受企业级质量数据库的所有好处。与关系数据库索引，对于许多应用程序，Neo4j可以提供数量级的性能优势。&#xA;Neo4j的安装与配置 Neo4j是基于Java的图形数据库，运行Neo4j需要启动JVM进程，因此在安装Neo4j前必须安装JAVA SE的JDK。从Oracle官方网站下载Java SE JDK， 地址为： https://www.oracle.com/cn/java/technologies/javase-downloads.html&#xA;Java JDK安装 1.双击.exe安装包（当前的最新版本） 2.根据向导安装 3.配置环境变量 编辑系统变量区的Path，点击新建，然后输入 %JAVA_HOME%\bin 4.测试 打开命令提示符CMD(WIN+R,输入cmd)，输入 java -version，若提示Java的版本信息，则证明环境变量配置成功&#xA;Neo4j的安装和配置&#xA;1.下载安装包&#xA;官网下载地址：https://neo4j.com/&#xA;2.解压&#xA;3.配置环境变量(和Java 环境变量设置步骤一致) 将下载的压缩文件解压到系统合适的位置后需要创建主目录环境变量NEO4J_HOME，变量值设置为主目录路径。图是主目录路径为C:\Users\ChangjianPan\Downloads\neo4j-community-5.22.0-windows\neo4j-community-5.22.0 的环境变量。&#xA;编辑系统变量区的Path，点击新建，然后输入 %NEO4J_HOME%\bin&#xA;Neo4j服务器具有一个集成的浏览器，在一个运行的服务器实例上访问 http://localhost:7474/&#xA;打开浏览器，显示启动页面。&#xA;在如图命令行依靠neo4j cli命令操作数据库 启动neo4j数据库服务器&#xA;通过使用neo4j.bat文件启动服务器，该文件存放在主目录的bin文件夹中 在cmd中，输入neo4j.bat console并回车&#xA;出现下图结果，则表示成功启动 123456Aa&#xA;忘记neo4j密码怎么办&#xA;找到neo4j.config文件 将 dbms.security.auth_enabled=false的注释去除&#xA;这样再次启动neo4j数据库服务器，以无密码身份登录neo4j数据库&#xA;然后在命令行窗口运行命令&#xA;ALTER USER neo4j SET PASSWORD &amp;lsquo;新密码&amp;rsquo;;&#xA;HuggingFists是什么&#xA;伴随着LLM日新月异的发展，业界对于LLM的落地思考逐渐聚焦到两个方向上。一是RAG(Retrieval-Augmented Generation),检索增强生成；一是Agents, 智能体。我们这个系列的文章也将围绕这两个应用方向介绍如何使用HuggingFists进行落地实现。其社区版可通过以下链接获得(https://github.com/Datayoo/HuggingFists)。&#xA;什么是RAG RAG，检索增强生成，即大模型LLM在回答问题或生成文本时，通过外挂其他数据源的方式来增强 LLM 的能力。使用外挂数据源检索出相关信息，然后基于这些检索出的信息进行回答或生成文本，从而提高回答的质量。外挂数据源可以是向量数据库、文档数据库、搜索引擎或应用系统等。RAG技术使得LLM在垂直领域应用时，不需要重新训练整个大模型，只需要外挂上相关知识库就可提供问答服务。从而节省了大模型的实施成本，同时提高了大模型在垂直领域的应用的时效性、准确性和安全性。&#xA;RAG工程面临的问题&#xA;基于向量数据库的RAG结构图&#xA;上图为基于向量数据库实现RAG的典型结构。除该结构外，业界还有基于ElasticSearch或二者共用的工程结构。本节将主要探讨所有工程结构都会面临的两个通用问题，一是RAG工程的技术选型问题；二是RAG工程的垂域数据接入问题。&#xA;RAG工程选定工程结构后，仍有大量的技术方案需要根据工程环境进行技术选型。如工程为云环境，则可以使用云端的向量数据库、LLM及Embedding技术；若工程要求本地化部署，且不具备外网服务访问条件，则需要选择可本地部署的向量库、LLM及Embedding等技术。如今，可供选择的技术方案有很多，工程师需要消耗不小的精力来试验并搭建出合适的选择。另外，如何优化检索也需要不断尝试。需要找出合适的方法来提高检索结果与问题的相关性，控制检索结果的大小以适配LLM的Context限制及token消耗。在找到合适的方案前，这些都将消耗不小的精力。 RAG工程待处理的垂域数据种类、格式繁多，有效提取这些数据并存入检索系统工作量巨大。这类工作其本质就是一个ETL过程。与传统的数据分析类项目相似，垂域数据预处理的工作量将占据整个项目的70%～80%。差别在于，传统数据分析项目面向的是结构化数据，而RAG项目面向的则更多是非结构化的数据，且情况更复杂。如相关数据是以Json或XML等格式描述的，且被存储在了如HBase、MongoDB等数据库中；文档结构比较复杂，无法简单通过文本抽取获得文档内容，而是需要文本抽取+OCR识别等多种技术相结合等。这些情况无疑都会加大RAG工程的落地成本。&#xA;解决上述问题，相信绝大多数LLM的关注者会首先想到LainChain。的确，LainChain无疑是目前实现LLM应用最炙手可热的框架。其采用Python语言作为开发语言，简单易用，拥有大量拥趸。自诞生以来，发展迅猛，且仍在不断完善中。&#xA;但有如我们在上述问题的描述中所说，RAG工程技术选型时有很多试验工作，垂域数据接入时有复杂场景需要适配。这些工作都存有更灵活、更便捷、更易用的潜在需求。特别是垂域数据接入，在过去几十年的传统数据接入工程中，存在大量低代码定义数据处理Pipeline的工具，如Kettle、StreamSets等。这些工具为复杂场景的垂域数据接入提供了很好的方案指导，可大大提高数据接入的效率，降低数据接入人员的编程能力要求。&#xA;基于此，HuggingFists应运而生，其可被视为是面向AI领域的低代码应用框架。力图成为LainChain的可视化平替，能够全面支持非结构化数据的ETL处理以及包括LLM在内的各类模型的应用。同时，其也支持作业调度，可确保应用与生产环境的集成。HuggingFists与LainChain一样，目前也在持续完善中，真正做到LainChain的平替尚待时日。&#xA;下载Huggingfist</description>
    </item>
    <item>
      <title>6_关于LangChain的调研</title>
      <link>http://localhost:1313/docs/ali/6_%E5%85%B3%E4%BA%8Elangchain%E7%9A%84%E8%B0%83%E7%A0%94/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/6_%E5%85%B3%E4%BA%8Elangchain%E7%9A%84%E8%B0%83%E7%A0%94/</guid>
      <description>一、什么是LangChain？ LangChain是一个强大的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由大型语言模型 (LLM) 和聊天模型提供支持的应用程序的过程。LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源，例如 API 和数据库。&#xA;官方文档：https://python.langchain.com/en/latest/&#xA;中文文档：https://www.langchain.com.cn/&#xA;LangChain本身并不开发LLMs，它的核心理念是为各种LLMs提供通用的接口，降低开发者的学习成本，方便开发者快速地开发复杂的LLMs应用。&#xA;官方的定义：LangChain是一个基于语言模型开发应用程序的框架。它可以实现以下应用程序：&#xA;数据感知：将语言模型连接到其他数据源 自主性：允许语言模型与其环境进行交互 主要价值在于：&#xA;组件化：为使用语言模型提供抽象层，以及每个抽象层的一组实现。组件是模块化且易于使用的，无论您是否使用LangChain框架的其余部分。 现成的链：结构化的组件集合，用于完成特定的高级任务 现成的链使得入门变得容易。对于更复杂的应用程序和微妙的用例，组件化使得定制现有链或构建新链变得更容易。&#xA;要使用 LangChain，开发人员首先要导入必要的组件和工具，例如 LLMs, chat models, agents, chains, 内存功能。这些组件组合起来创建一个可以理解、处理和响应用户输入的应用程序。&#xA;LangChain 为特定用例提供了多种组件，例如个人助理、文档问答、聊天机器人、查询表格数据、与 API 交互、提取、评估和汇总。&#xA;二、模型分类和特点 LangChain model 是一种抽象，表示框架中使用的不同类型的模型。&#xA;LangChain 中的模型分类：&#xA;LLM（大型语言模型）：这些模型将文本字符串作为输入并返回文本字符串作为输出。它们是许多语言模型应用程序的支柱。 聊天模型( Chat Model)：聊天模型由语言模型支持，但具有更结构化的 API。他们将聊天消息列表作为输入并返回聊天消息。这使得管理对话历史记录和维护上下文变得容易。 文本嵌入模型(Text Embedding Models)：这些模型将文本作为输入并返回表示文本嵌入的浮点列表。这些嵌入可用于文档检索、聚类和相似性比较等任务。&#xA;LangChain 的特点：&#xA;LLM 和提示：LangChain 使管理提示、优化它们以及为所有 LLM 创建通用界面变得容易。此外，它还包括一些用于处理 LLM 的便捷实用程序。&#xA;链(Chain)：这些是对 LLM 或其他实用程序的调用序列。LangChain 为链提供标准接口，与各种工具集成，为流行应用提供端到端的链。&#xA;数据增强生成：LangChain 使链能够与外部数据源交互以收集生成步骤的数据。例如，它可以帮助总结长文本或使用特定数据源回答问题。&#xA;Agents：Agents 让 LLM 做出有关行动的决定，采取这些行动，检查结果，并继续前进直到工作完成。LangChain 提供了代理的标准接口，多种代理可供选择，以及端到端的代理示例。 内存：LangChain 有一个标准的内存接口，有助于维护链或代理调用之间的状态。它还提供了一系列内存实现和使用内存的链或代理的示例。&#xA;评估：很难用传统指标评估生成模型。这就是为什么 LangChain 提供提示和链来帮助开发者自己使用 LLM 评估他们的模型。</description>
    </item>
    <item>
      <title>7_基于Langchain和ollama构建的本地RAG雏形</title>
      <link>http://localhost:1313/docs/ali/7_%E5%9F%BA%E4%BA%8Elangchain%E5%92%8Collama%E6%9E%84%E5%BB%BA%E7%9A%84%E6%9C%AC%E5%9C%B0rag%E9%9B%8F%E5%BD%A2/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/7_%E5%9F%BA%E4%BA%8Elangchain%E5%92%8Collama%E6%9E%84%E5%BB%BA%E7%9A%84%E6%9C%AC%E5%9C%B0rag%E9%9B%8F%E5%BD%A2/</guid>
      <description>效果展示&#xA;首先用ai虚构了一个修仙短篇故事，以沁三郎为主人公，写一个凡人修仙的故事&#xA;然后把这个故事存成san.txt文件&#xA;测试1：测试模型是否能根据提供的文本内容回答问题 向模型提问：&amp;ldquo;沁三郎住哪里？&amp;rdquo;&#xA;回答：&amp;ldquo;沁三郎居住在遥远东方的一片名为青云大陆的地方，这里山川秀美，灵气充沛，是修炼者梦寐以求的圣地。&amp;rdquo;&#xA;这和提供的文本内容基本一致&#xA;测试2：测试模型是否能根据提供的文本内容综合回答问题 向模型提问：&amp;ldquo;沁三郎修炼的功法是什么？&amp;rdquo;&#xA;回答： &amp;ldquo;《灵元诀》和《青云诀》是沁三郎修炼的功法。&amp;rdquo;&#xA;这和提供的文本内容基本一致&#xA;测试3：测试模型是否能回答文本之外的问题&#xA;向模型提问：&amp;ldquo;沁三郎喜欢肯德基还是蜜雪冰城&amp;rdquo;&#xA;回答： &amp;ldquo;沁三郎对修仙之道充满了好奇和热情，因此他的饮食习惯可能与修炼者的日常饮食有所差异，但无法确定他是否特别喜爱肯德基或蜜雪冰城。在《凡人成仙录》的故事情节中，沁三郎更专注于自己的修行之路，并没有提及他对食品的选择偏好。&amp;rdquo;&#xA;这体现了模型对文本之外问题的理解能力&#xA;下面是这个demo的完整代码&#xA;from langchain_community.document_loaders import TextLoader&#xD;from langchain.text_splitter import RecursiveCharacterTextSplitter&#xD;from langchain_huggingface import HuggingFaceEmbeddings&#xD;from langchain_community.vectorstores import FAISS&#xD;from langchain.chains import RetrievalQA&#xD;from langchain_community.llms import Ollama&#xD;from fastapi import FastAPI&#xD;from pydantic import BaseModel&#xD;import uvicorn&#xD;# 1. 文档加载与文本分割&#xD;def load_and_split_documents(file_path: str):&#xD;loader = TextLoader(file_path, encoding=&amp;#34;utf-8&amp;#34;)&#xD;documents = loader.load()&#xD;# 使用文本分割器进行分割&#xD;text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)&#xD;split_docs = text_splitter.</description>
    </item>
    <item>
      <title>8_基于ollama对RAG的调用来构建neo4j的知识图谱</title>
      <link>http://localhost:1313/docs/ali/8_%E5%9F%BA%E4%BA%8Eollama%E5%AF%B9rag%E7%9A%84%E8%B0%83%E7%94%A8%E6%9D%A5%E6%9E%84%E5%BB%BAneo4j%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/8_%E5%9F%BA%E4%BA%8Eollama%E5%AF%B9rag%E7%9A%84%E8%B0%83%E7%94%A8%E6%9D%A5%E6%9E%84%E5%BB%BAneo4j%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</guid>
      <description>基本配置：&#xA;olllama+qwen2:1.5b&#xA;ollama serve 查看ollama的启动情况&#xA;当前为未连接状态，需要连接后才能使用&#xA;ollama list 查看已经安装好的本地模型&#xA;ollama run qwen2:1.5b 启动模型&#xA;ollama ps 查看正在运行的模型&#xA;neo4j的启动&#xA;为了方便可以直接配置环境变量&#xA;然后就运行cmd&#xA;neo4j.bat console&#xA;出现如图内容后，说明启动成功&#xA;进入&#xA;http://localhost:7474/&#xA;输入账号，密码&#xA;到达界面&#xA;准备数据材料&#xA;启动py文件&#xA;运行结果&#xA;在neo4j数据库查询&#xA;MATCH (n)&#xA;RETURN n&#xA;代码如下&#xA;import json&#xD;import re&#xD;import os&#xD;import jieba&#xD;from langchain_community.llms import Ollama&#xD;from neo4j import GraphDatabase&#xD;# Neo4j连接配置&#xD;NEO4J_URI = &amp;#34;bolt://localhost:7687&amp;#34;&#xD;NEO4J_USER = &amp;#34;neo4j&amp;#34;&#xD;NEO4J_PASSWORD = &amp;#34;123456Aa&amp;#34;&#xD;# Ollama API配置&#xD;OLLAMA_MODEL = &amp;#34;qwen2:1.5b&amp;#34;&#xD;# 文件夹路径&#xD;FOLDER_PATH = r&amp;#34;C:\Users\ChangjianPan\OneDrive - Kongsberg Digital AS\Desktop\三创\ai-backend\backend\mysite\chat\ragkm&amp;#34;&#xD;# 1.</description>
    </item>
    <item>
      <title>9_Huggingface transformers文本生成</title>
      <link>http://localhost:1313/docs/ali/9_huggingface-transformers%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/9_huggingface-transformers%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/</guid>
      <description>文章地址&#xA;https://fancyerii.github.io/2023/12/19/hg-transformer-generate/&#xA;本文整理了Huggingface transformers文本生成相关的资料。&#xA;目录&#xA;文本生成策略 默认文本生成策略 定制文本生成 保存自定义解码策略与您的模型 Streaming 解码策略 贪婪搜索(Greedy Search) 对比搜索(Contrastive search) 多项式采样(Multinomial sampling) Beam搜索解码(Beam-search decoding) Beam搜索多项式采样(Beam-search multinomial sampling) 多样性beam搜索(Diverse beam search decoding) 辅助解码(Assisted Decoding) 相关代码&#xA;约束的Beam搜索(Constrained Beam Search) 贪婪搜索 对比搜索 多项式采样 Beam搜索 多样性Beam搜索(Group Beam Search) 多项式采样Beam搜索 辅助搜索 8.1 语言模型解码回顾 8.2 使用辅助搜索的贪心解码算法 参考资料 文本生成策略 文本生成对于许多自然语言处理（NLP）任务至关重要，例如开放式文本生成、摘要、翻译等。它还在许多混合模态应用中发挥作用，这些应用以文本作为输出，如语音转文字和视觉转文字。一些能够生成文本的模型包括GPT2、XLNet、OpenAI GPT、CTRL、TransformerXL、XLM、Bart、T5、GIT、Whisper。 请注意，generate方法的输入取决于模型的模态性。这些输入由模型的预处理器类（如AutoTokenizer或AutoProcessor）返回。如果模型的预处理器创建了多种类型的输入，请将所有输入传递给generate()。您可以在相应模型的文档中了解有关各个模型预处理器的更多信息。&#xA;选择生成文本的输出token的过程称为解码，您可以自定义generate()方法将使用的解码策略。修改解码策略不会更改任何可训练参数的值。然而，它可能会显著影响生成输出的质量，有助于减少文本中的重复并使其更连贯。&#xA;默认文本生成策略 模型的解码策略是在其生成配置中定义的。在使用预训练模型用pipeline进行生成时，模型调用PreTrainedModel.generate()方法，在幕后应用默认的生成配置。当没有使用自定义配置保存模型时，也会使用默认配置。&#xA;当您显式加载一个模型时，您可以通过model.generation_config检查生成配置：&#xA;from transformers import AutoModelForCausalLM&#xA;model = AutoModelForCausalLM.from_pretrained(&amp;ldquo;distilgpt2&amp;rdquo;) model.generation_config 输出： GenerationConfig { &amp;ldquo;bos_token_id&amp;rdquo;: 50256, &amp;ldquo;eos_token_id&amp;rdquo;: 50256, } 打印model.</description>
    </item>
    <item>
      <title>10_x</title>
      <link>http://localhost:1313/docs/ali/10_x/</link>
      <pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/docs/ali/10_x/</guid>
      <description>继续调研部分分为理论调研和代码调研&#xA;代码调研&#xA;https://github.com/docker/genai-stack&#xA;该项目具有以下调研的优势&#xA;代码结构清晰，py文件用docker打包，和静态前端界面，FastAPI提供http请求,启动用sh脚本启动。 使用本地ollama作为模型，neo4j作为数据库，向量化存储，最终实现RAG+知识图谱上下文的综合搜素。 provide summarized answers with sources 提供带有来源的摘要答案 demonstrate difference between 展示两者之间的差异 RAG Disabled (pure LLM response) RAG 禁用（纯LLM响应） RAG Enabled (vector + knowledge graph context) RAG 已启用（向量 + 知识图谱上下文） PDF文件作为数据来源 代码调研</description>
    </item>
  </channel>
</rss>
